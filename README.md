# rag-chat-with-pdf
Simple Chainlit UI for running llms locally using Ollama and LangChain

#python3 -m venv .venv && source .venv/bin/activate

#curl -fsSL https://ollama.com/install.sh | sh

#ollama run mistral:instruct

#ollama run nomic:embeded-text

#chainlit run app.py

